# RunArguments
DEBUG: no
k_folds: 5
num_proc: 7
max_seq_length: 
padding: no
stride: 0
model_name_or_path: "kisholas/deb-v3-l-idpt-newline-repl"
data_dir: "./data"
min_score_to_save: 0.88
masking_prob: 0.1
reinit_layers: 0
layer_norm_eps: 1e-7
dropout: 0.1
use_crf: no
loss: 'bce'
use_8bit: no
use_layerwise_lr: no
layer_lr_alpha: 0.8
use_lookahead: no
use_swa: no
swa_lr: 1e-7
swa_start: 0.8
swa_steps: 50
use_gated: no
gated_act_fn: "gelu_new"
use_sift: no
newline_replacement: " [n] "
space_token: "â–"
n_gpu: 1
use_pseudolabels: "pl_fold0.csv"

# frozen
n_frozen_layers: 0
freeze_embeds: no

# WandB Config
project: nbme-real
entity: nbroad
group: fine-tune
tags:
  - ner
  - deb-v3-l
  - pl
notes: >
  deb v3 large finetune
  cleaned data
  pseudolabels
job_type: train

# TrainingArguments
# <default> means use default value
training_arguments:
# output
  output_dir: "./"
  overwrite_output_dir: no

# dataloader
  dataloader_num_workers: 2
  dataloader_pin_memory: yes

# training
  do_train: yes
  resume_from_checkpoint:
  seed: 18

# hyperparams
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  gradient_checkpointing: no
  group_by_length: yes
  learning_rate: 8e-6
  weight_decay: .01

# schedule + steps
  num_train_epochs: 5
  lr_scheduler_type: linear
  warmup_ratio: 0.1
  warmup_steps: 0
  max_steps: -1

# optimizer
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-6
  max_grad_norm: 1.0
  optim: 'adamw_hf'
  adafactor: no

# logging
  log_level: "warning"
  log_level_replica: <default>
  log_on_each_node: <default>
  logging_dir: <default>
  logging_strategy: steps
  logging_first_step: no
  logging_steps: 100
  logging_nan_inf_filter: <default>

# dtype
  bf16: no
  fp16: yes
  fp16_opt_level: "O1"
  half_precision_backend: "auto"
  bf16_full_eval: no
  fp16_full_eval: no
  tf32: no

# saving (will be handled by callback)
  save_strategy: "no"
  save_steps: 300000
  save_total_limit: 1
  load_best_model_at_end: no
  metric_for_best_model: "eval_f1"

# evaluation
  do_eval: yes
  evaluation_strategy: "epoch"
  eval_steps: 125
  eval_delay: 0
  per_device_eval_batch_size: 32
  do_predict:
  
# hub
  push_to_hub: yes
  hub_model_id:
  hub_strategy: "every_save"
  hub_token:

# misc
  report_to: "wandb"
  hub_private_repo: yes

# rarely used
  debug: <default>
  prediction_loss_only: <default>
  per_gpu_train_batch_size: <default>
  per_gpu_eval_batch_size: <default>
  eval_accumulation_steps: <default>
  save_on_each_node: <default>
  no_cuda: <default>
  local_rank: <default>
  xpu_backend: <default>
  tpu_num_cores: <default>
  tpu_metrics_debug: <default>
  dataloader_drop_last: <default>
  past_index: <default>
  run_name: <default>
  disable_tqdm: <default>
  remove_unused_columns: <default>
  label_names: <default>
  greater_is_better: <default>
  ignore_data_skip: <default>
  sharded_ddp: <default>
  deepspeed: <default>
  label_smoothing_factor: <default>
  length_column_name: <default>
  ddp_find_unused_parameters: <default>
  ddp_bucket_cap_mb: <default>
  skip_memory_metrics: <default>
  use_legacy_prediction_loop: <default>
  fp16_backend: <default>
  mp_parameters: <default>